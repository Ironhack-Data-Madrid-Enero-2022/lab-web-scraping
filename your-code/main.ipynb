{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some web scraping exercises to practice your scraping skills using `requests` and `Beautiful Soup`.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the [response status code](https://http.cat/) for each request to ensure you have obtained the intended content.\n",
    "- Look at the HTML code in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract.\n",
    "- Check out the css selectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Resources\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all, gathering our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Again, please remember to limit your output before submission so that your code doesn't get lost in the output.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 1 - Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'\n",
    "html = req.get(url).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = soup.find_all('h1',{'class':\"h3 lh-condensed\"})\n",
    "user = soup.find_all('p',{'class':\"f4 text-normal mb-1\"})\n",
    "developers =[]\n",
    "for i in range(0,25):\n",
    "    dev_name = dev[i].text.strip()\n",
    "    developers.append(dev_name)\n",
    "    #user_name = user[i].text.strip() # some users don't have a user name \n",
    "    #print(dev_name, user_name)\n",
    "# I don't know how to relate both names and \"user names\" as some of the developers don't have one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below (with different names):\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yann Collet (Cyan4973)', 'Arvid Norberg (arvidn)', 'Emil Ernerfeldt (emilk)', 'Mathias Fußenegger (mfussenegger)', 'David Peter (sharkdp)', 'Koen Kanters (Koenkk)', 'Markus Unterwaditzer (untitaker)', 'Mike Penz (mikepenz)', 'LoveSy (yujincheng08)', 'David Sherret (dsherret)', 'mrdoob', 'Joe Previte (jsjoeio)', 'Earle F. Philhower, III (earlephilhower)', 'MichaIng', 'Chris Caron (caronc)', 'Jaco (jacogr)', 'vis2k', 'Max Desiatov (MaxDesiatov)', 'Stephan Dilly (extrawurst)', 'Timothy (timlrx)', 'Shu Ding (shuding)', 'Jonathan Neal (jonathantneal)', 'Gwendal Roué (groue)', 'Mikael Finstad (mifi)', 'Jared Palmer (jaredpalmer)']\n"
     ]
    }
   ],
   "source": [
    "# Found the way to do what I couldn't in the last challenge, \n",
    "dev = soup.find_all('div',{'class':\"col-md-6\"})\n",
    "developers = [developer.text.strip().split('\\n') for developer in dev]\n",
    "#I realized that we only one 1 every 2 elements in the list, as every other element is related to the previous user, plus I want to put the elements in a dict,\n",
    "dev_dict = dict()\n",
    "count = 0\n",
    "for developer in developers:\n",
    "    if count%2 == 0: # we are just taking 1 every 2 elements\n",
    "        llista = [i.strip() for i in developers[count] if i not in ['',' ']]\n",
    "        dev_name = llista[0]\n",
    "        if len(llista) > 1:\n",
    "            user_name = llista[1]\n",
    "        else:\n",
    "            user_name = ''\n",
    "        dev_dict[dev_name] = user_name\n",
    "    count += 1\n",
    "# Now we can already print that list,\n",
    "result = []\n",
    "for i in dev_dict:\n",
    "    if dev_dict[i] != '':\n",
    "        to_append = i+' ('+dev_dict[i]+')'\n",
    "        result.append(to_append)\n",
    "    else:\n",
    "        result.append(i)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Challenge 2 - Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "html = req.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RunaCapital/awesome-oss-alternatives', 'Jxck-S/plane-notify', 'freqtrade/freqtrade-strategies', 'home-assistant/core', 'sherlock-project/sherlock', 'ytdl-org/youtube-dl', 'yt-dlp/yt-dlp', 'TheAlgorithms/Python', 'slingamn/mureq', 'zhzyker/exphub', 'bilibili/ailab', 'vinta/awesome-python', 'feast-dev/feast', 'kivy/kivy', 'psf/black', 'Chia-Network/chia-blockchain', 'github/copilot-docs', 'nonebot/nonebot2', 'rougier/scientific-visualization-book', 'miguelgrinberg/flasky', 'binance/binance-connector-python', 'arc298/instagram-scraper', 'patrikzudel/PatrikZeros-CSGO-Sound-Fix', 'NotReallyShikhar/YukkiMusicBot', 'KurtBestor/Hitomi-Downloader']\n"
     ]
    }
   ],
   "source": [
    "soup=bs(html, 'html.parser')\n",
    "rep = soup.find_all('h1',{'class':'h3 lh-condensed'}) # It is actually the very same thing than before\n",
    "repos =[i.text.replace('\\n', '').replace(' ','') for i in rep]\n",
    "print(repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 3 - Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "html = req.get(url).content\n",
    "soup=bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images= soup.find_all('img')\n",
    "links_img = [image['src'] for image in images]\n",
    "links_img[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 4 - Retrieve all links to pages on Wikipedia that refer to some kind of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' \n",
    "html = req.get(url).content\n",
    "soup=bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wiktionary.org/wiki/python',\n",
       " '/wiki/Pythonidae']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links= soup.find_all('a', href=True)\n",
    "links_list = []\n",
    "for a in links:\n",
    "    if 'wiki' in a['href']:\n",
    "        links_list.append(a['href'])\n",
    "links_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 5 - Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "html = req.get(url).content\n",
    "soup=bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changedtitles= soup.find_all('div', class_= \"usctitlechanged\")\n",
    "len(changedtitles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 6 - A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "html = req.get(url).content\n",
    "soup=bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YULAN ADONAY ARCHAGA CARIAS', 'EUGENE PALMER', 'BHADRESHKUMAR CHETANBHAI PATEL', 'ALEJANDRO ROSALES CASTILLO', 'ARNOLDO JIMENEZ', 'JASON DEREK BROWN', 'ALEXIS FLORES', 'JOSE RODOLFO VILLARREAL-HERNANDEZ', 'OCTAVIANO JUAREZ-CORRO', 'RAFAEL CARO-QUINTERO']\n"
     ]
    }
   ],
   "source": [
    "fugitive_object= soup.find_all('div', class_= \"focuspoint\")\n",
    "FBI_list = []\n",
    "for i in fugitive_object:\n",
    "    for j in i: FBI_list.append(j['alt'])\n",
    "\n",
    "print(FBI_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 7 - List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "html = req.get(url).content\n",
    "soup=bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English': '6.383.000', 'Nihongo': '1.292.000', 'Russkiy': '1.756.000', 'Deutsch': '2.617.000', 'Español': '1.717.000', 'Français': '2.362.000', 'Italiano': '1.718.000', 'Zhōngwén': '1.231.000', 'Polski': '1.490.000', 'Português': '1.074.000'}\n"
     ]
    }
   ],
   "source": [
    "language_obj = soup.find_all('div', class_= \"central-featured-lang\")\n",
    "lenguage_dict= dict()\n",
    "for lenguage in language_obj:\n",
    "    Idioma = lenguage.find('a').attrs['title']\n",
    "    Idioma = Idioma.split('—')[0].strip()\n",
    "    num_art = lenguage.find('bdi').text.strip('+')\n",
    "    lenguage_dict[Idioma] = num_art.replace(u\"\\xa0\",'.')\n",
    "print(lenguage_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 8 - A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "html = req.get(url).content\n",
    "soup=bs(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Business and economy', 'Crime and justice', 'Defence', 'Education', 'Environment', 'Government', 'Government spending', 'Health', 'Mapping', 'Society', 'Towns and cities', 'Transport', 'Digital service performance', 'Government reference data']\n"
     ]
    }
   ],
   "source": [
    "data_set_kind = soup.find_all('h3',{'class':\"govuk-heading-s dgu-topics__heading\"})\n",
    "data_sets_list = []\n",
    "for kind in data_set_kind:\n",
    "    data_sets_list.append(kind.text.strip())\n",
    "print(data_sets_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 9 - Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "html = req.get(url).content\n",
    "soup=bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers(millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi (sanskritised Hindustani)[11]</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Western Punjabi[12]</td>\n",
       "      <td>92.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Telugu</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Language Speakers(millions)\n",
       "0                      Mandarin Chinese                918\n",
       "1                               Spanish                480\n",
       "2                               English                379\n",
       "3   Hindi (sanskritised Hindustani)[11]                341\n",
       "4                               Bengali                300\n",
       "5                            Portuguese                221\n",
       "6                               Russian                154\n",
       "7                              Japanese                128\n",
       "8                   Western Punjabi[12]               92.7\n",
       "9                               Marathi               83.1\n",
       "10                               Telugu               82.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table =soup.find_all('table')[1]\n",
    "rows=table.find_all('tr')\n",
    "\n",
    "rows=[row.text.strip().split('\\n') for row in rows]\n",
    "\n",
    "top10 = rows[0:12]\n",
    "fnl = []\n",
    "for language in top10:\n",
    "    fnl.append( [language[2], language[4]])\n",
    "\n",
    "col_names=fnl[0]\n",
    "\n",
    "data=fnl[1:12]\n",
    "\n",
    "df=pd.DataFrame(data, columns=col_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping up the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Challenge 10 - The 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "html = req.get(url).content\n",
    "soup=bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Region Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>07:40:43</td>\n",
       "      <td>36.47N</td>\n",
       "      <td>121.04W</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>07:25:26</td>\n",
       "      <td>15.33N</td>\n",
       "      <td>91.90W</td>\n",
       "      <td>GUATEMALA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>06:48:10</td>\n",
       "      <td>11.25S</td>\n",
       "      <td>73.39W</td>\n",
       "      <td>CENTRAL PERU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>06:42:41</td>\n",
       "      <td>28.54N</td>\n",
       "      <td>17.87W</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>06:31:25</td>\n",
       "      <td>34.40N</td>\n",
       "      <td>25.55E</td>\n",
       "      <td>CRETE, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>06:18:33</td>\n",
       "      <td>28.56N</td>\n",
       "      <td>17.82W</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>06:16:32</td>\n",
       "      <td>28.56N</td>\n",
       "      <td>17.84W</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>06:06:59</td>\n",
       "      <td>39.16N</td>\n",
       "      <td>40.09E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>06:03:11</td>\n",
       "      <td>36.37N</td>\n",
       "      <td>7.60W</td>\n",
       "      <td>STRAIT OF GIBRALTAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>05:33:50</td>\n",
       "      <td>35.44N</td>\n",
       "      <td>3.67W</td>\n",
       "      <td>STRAIT OF GIBRALTAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>05:25:16</td>\n",
       "      <td>19.17N</td>\n",
       "      <td>155.49W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>05:24:04</td>\n",
       "      <td>18.80N</td>\n",
       "      <td>74.01W</td>\n",
       "      <td>HAITI REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>05:20:36</td>\n",
       "      <td>38.25N</td>\n",
       "      <td>38.79E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>05:18:53</td>\n",
       "      <td>36.13S</td>\n",
       "      <td>177.98E</td>\n",
       "      <td>OFF E. COAST OF N. ISLAND, N.Z.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>05:07:33</td>\n",
       "      <td>19.21N</td>\n",
       "      <td>155.42W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>05:06:28</td>\n",
       "      <td>31.30S</td>\n",
       "      <td>68.91W</td>\n",
       "      <td>SAN JUAN, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>04:38:56</td>\n",
       "      <td>10.71S</td>\n",
       "      <td>111.18E</td>\n",
       "      <td>SOUTH OF JAVA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>04:18:17</td>\n",
       "      <td>36.08S</td>\n",
       "      <td>178.10E</td>\n",
       "      <td>OFF E. COAST OF N. ISLAND, N.Z.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>03:54:37</td>\n",
       "      <td>37.06N</td>\n",
       "      <td>5.33W</td>\n",
       "      <td>SPAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>03:52:24</td>\n",
       "      <td>35.46N</td>\n",
       "      <td>3.66W</td>\n",
       "      <td>STRAIT OF GIBRALTAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>03:42:39</td>\n",
       "      <td>39.14N</td>\n",
       "      <td>40.05E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Time Latitude Longitude                      Region Name\n",
       "0   2022-01-31  07:40:43   36.47N   121.04W               CENTRAL CALIFORNIA\n",
       "1   2022-01-31  07:25:26   15.33N    91.90W                        GUATEMALA\n",
       "2   2022-01-31  06:48:10   11.25S    73.39W                     CENTRAL PERU\n",
       "3   2022-01-31  06:42:41   28.54N    17.87W     CANARY ISLANDS, SPAIN REGION\n",
       "4   2022-01-31  06:31:25   34.40N    25.55E                    CRETE, GREECE\n",
       "5   2022-01-31  06:18:33   28.56N    17.82W     CANARY ISLANDS, SPAIN REGION\n",
       "6   2022-01-31  06:16:32   28.56N    17.84W     CANARY ISLANDS, SPAIN REGION\n",
       "7   2022-01-31  06:06:59   39.16N    40.09E                   EASTERN TURKEY\n",
       "8   2022-01-31  06:03:11   36.37N     7.60W              STRAIT OF GIBRALTAR\n",
       "9   2022-01-31  05:33:50   35.44N     3.67W              STRAIT OF GIBRALTAR\n",
       "10  2022-01-31  05:25:16   19.17N   155.49W         ISLAND OF HAWAII, HAWAII\n",
       "11  2022-01-31  05:24:04   18.80N    74.01W                     HAITI REGION\n",
       "12  2022-01-31  05:20:36   38.25N    38.79E                   EASTERN TURKEY\n",
       "13  2022-01-31  05:18:53   36.13S   177.98E  OFF E. COAST OF N. ISLAND, N.Z.\n",
       "14  2022-01-31  05:07:33   19.21N   155.42W         ISLAND OF HAWAII, HAWAII\n",
       "15  2022-01-31  05:06:28   31.30S    68.91W              SAN JUAN, ARGENTINA\n",
       "16  2022-01-31  04:38:56   10.71S   111.18E         SOUTH OF JAVA, INDONESIA\n",
       "17  2022-01-31  04:18:17   36.08S   178.10E  OFF E. COAST OF N. ISLAND, N.Z.\n",
       "18  2022-01-31  03:54:37   37.06N     5.33W                            SPAIN\n",
       "19  2022-01-31  03:52:24   35.46N     3.66W              STRAIT OF GIBRALTAR\n",
       "20  2022-01-31  03:42:39   39.14N    40.05E                   EASTERN TURKEY"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table =soup.find_all('table')[3]\n",
    "rows = table.find_all('tr')\n",
    "rows=[row.text.strip().split('\\n') for row in rows]\n",
    "final=[]\n",
    "for row in rows:\n",
    "    tmp=[]\n",
    "    for st in row:\n",
    "        if st!='':\n",
    "            tmp.append(st)\n",
    "    final.append(tmp)\n",
    "# Now we need to clean it a little further and then take the first 20 rows of data, as we are going to rewrite the head of the table\n",
    "definitiu = []\n",
    "for i in range(5,26):\n",
    "    row = final[i][0].split(u'\\xa0')\n",
    "    row  = [wrd for wrd in row if wrd != '']\n",
    "    date, time1, latitude, dir, longitude, dir2, region_name = row\n",
    "    date = re.findall('\\d{4}-\\d{2}-\\d{2}', date)[0]\n",
    "    time = re.findall('\\d{2}:\\d{2}:\\d{2}', time1)[0]\n",
    "    latitude = re.findall('\\d{1,2}.\\d{2}$', time1)[0]+latitude\n",
    "    longitude = dir+longitude\n",
    "    region_name = region_name[:-16]\n",
    "    definitiu.append([date,time,latitude,longitude,region_name])\n",
    "\n",
    "    #definitiu.append() \n",
    "col_names = ['Date', 'Time', 'Latitude', 'Longitude', 'Region Name']\n",
    "df=pd.DataFrame(definitiu, columns=col_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 11 - IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "html = req.get(url).content\n",
    "soup=bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "table =soup.find_all('tr')\n",
    "film_list = []\n",
    "counter = 0\n",
    "for film in table:\n",
    "    if counter > 0:\n",
    "        lista_titles_rates = film.text.split('\\n')\n",
    "        lista_titles_rates = [i for i in lista_titles_rates if i not in ['',' ',',',', ']]\n",
    "        title =lista_titles_rates[1].strip()\n",
    "        rate = lista_titles_rates[3]\n",
    "        film_list.append([title, rate])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "table =soup.find_all('tr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 12 - Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 13 - Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(city):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 14 - Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did you limit your output? Thank you! 🙂**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
