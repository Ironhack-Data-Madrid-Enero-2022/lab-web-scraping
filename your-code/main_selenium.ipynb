{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some web scraping exercises to practice your scraping skills using `requests` and `Beautiful Soup`.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the [response status code](https://http.cat/) for each request to ensure you have obtained the intended content.\n",
    "- Look at the HTML code in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract.\n",
    "- Check out the css selectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Resources\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all, gathering our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Driver options:\n",
    "opciones=Options()\n",
    "opciones.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "opciones.add_experimental_option('useAutomationExtension', True)\n",
    "opciones.headless=True    # Chrome window won show\n",
    "opciones.add_argument('--start-maximized')\n",
    "opciones.add_argument('--incognito')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Again, please remember to limit your output before submission so that your code doesn't get lost in the output.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 1 - Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 98.0.4758\n",
      "Get LATEST chromedriver version for 98.0.4758 google-chrome\n",
      "Driver [/home/vp/.wdm/drivers/chromedriver/linux64/98.0.4758.80/chromedriver] found in cache\n",
      "/tmp/ipykernel_158679/1975688665.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver=webdriver.Chrome(PATH, options=opciones) #Cargamos Driver y opciones\n"
     ]
    }
   ],
   "source": [
    "PATH=ChromeDriverManager().install()\n",
    "driver=webdriver.Chrome(PATH, options=opciones) #Cargamos Driver y opciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "URL = 'https://github.com/trending/developers'\n",
    "\n",
    "driver.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below (with different names):\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (Á•ûÊ•ΩÂùÇË¶ö„ÄÖ)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "devs_list = driver.find_elements(By.CLASS_NAME, 'Box-row') #lista de desarrolladores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Matthias Feyrusty1s',\n",
       " 'Yujia Qiaorapiz1',\n",
       " 'David Rodr√≠guezdeivid-rodriguez',\n",
       " 'Geoff Bourneitzg',\n",
       " 'Keno FischerKeno',\n",
       " 'Vladimir Agafonkinmourner',\n",
       " 'Jeremy Longjeremylong',\n",
       " 'Steve Gordonstevejgordon',\n",
       " 'Franck Nijhoffrenck',\n",
       " 'Pedro S. Lopezpedroslopez',\n",
       " 'Kazuaki MatsuoKazuCocoa',\n",
       " 'Juliettejrfnl',\n",
       " 'Seth Michael Larsonsethmlarson',\n",
       " 'Robert Mosolgormosolgo',\n",
       " 'Alex Goodmanwagoodman',\n",
       " 'Florian RothNeo23x0',\n",
       " 'Patrick Kidgerpatrick-kidger',\n",
       " 'Dries Vintsdriesvints',\n",
       " 'Sebasti√°n Ram√≠reztiangolo',\n",
       " 'Ëä±Ë£§Ë°©PanJiaChen',\n",
       " 'Sameer Naiksameersbn',\n",
       " 'Anthony Fuantfu',\n",
       " 'Willem Melchingpd0wm',\n",
       " 'Alex Rogozhnikovarogozhnikov',\n",
       " 'Sylvain Guggersgugger']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking = []\n",
    "for element in devs_list:\n",
    "    dev = f\"{element.find_element(By.CSS_SELECTOR,'div.d-sm-flex.flex-auto > div.col-sm-8.d-md-flex > div:nth-child(1) > h1').text}{element.find_element(By.CLASS_NAME,'Link--secondary').text}\"\n",
    "    ranking.append(dev)\n",
    "ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Challenge 2 - Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poe-archnemesis-scanner',\n",
       " 'manim',\n",
       " 'ailab',\n",
       " 'hosts',\n",
       " 'ivy',\n",
       " 'recommenders',\n",
       " 'youtube-dl',\n",
       " 'scientific-visualization-book',\n",
       " 'sh',\n",
       " 'pz',\n",
       " 'public-apis',\n",
       " 'rich',\n",
       " 'freqtrade',\n",
       " 'chia-blockchain',\n",
       " 'frame-interpolation',\n",
       " 'OSX-KVM',\n",
       " 'quantstats',\n",
       " 'zulip',\n",
       " 'pycryptobot',\n",
       " 'Python',\n",
       " 'magenta',\n",
       " 'awesome-quant',\n",
       " 'CtCI-6th-Edition-Python',\n",
       " 'calibre',\n",
       " 'system-design-primer']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repos = driver.find_elements(By.CLASS_NAME,'lh-condensed')\n",
    "ranking = []\n",
    "for element in repos:\n",
    "    quitar = element.find_element(By.TAG_NAME, 'span').text # To remove user 'name/'' from element.text\n",
    "    completo = element.find_element(By.TAG_NAME,'a').text #element.text\n",
    "    repo_limpio = completo.replace(quitar,'').lstrip()\n",
    "    ranking.append(repo_limpio)\n",
    "ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 3 - Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = driver.find_elements(By.TAG_NAME,'img')\n",
    "image_links = []\n",
    "for element in images:\n",
    "    image_links.append(element.get_attribute('src'))\n",
    "\n",
    "image_links[:10] #len(image_links) = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 4 - Retrieve all links to pages on Wikipedia that refer to some kind of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Pythonidae', 'https://en.wikipedia.org/wiki/Python_(genus)', 'https://en.wikipedia.org/wiki/Python#Computing', 'https://en.wikipedia.org/wiki/Python#People', 'https://en.wikipedia.org/wiki/Python#Roller_coasters', 'https://en.wikipedia.org/wiki/Python#Vehicles', 'https://en.wikipedia.org/wiki/Python#Weaponry', 'https://en.wikipedia.org/wiki/Python#Other_uses', 'https://en.wikipedia.org/wiki/Python#See_also', 'https://en.wikipedia.org/wiki/Python_(programming_language)', 'https://en.wikipedia.org/wiki/CMU_Common_Lisp', 'https://en.wikipedia.org/wiki/PERQ#PERQ_3', 'https://en.wikipedia.org/wiki/Python_of_Aenus', 'https://en.wikipedia.org/wiki/Python_(painter)', 'https://en.wikipedia.org/wiki/Python_of_Byzantium'],list length:130\n"
     ]
    }
   ],
   "source": [
    "lists = driver.find_elements(By.TAG_NAME,'li')\n",
    "links = []\n",
    "for element in lists:\n",
    "    try: #Not al li elements have an 'a' in it, or an href attribute\n",
    "        link = element.find_element(By.TAG_NAME,'a')\n",
    "        links.append(link.get_attribute('href'))\n",
    "    except:\n",
    "        pass\n",
    "print('{},list length:{}'.format(links[:15],len(links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 5 - Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title 1 - General Provisions Ÿ≠', 'Title 2 - The Congress', 'Title 5 - Government Organization and Employees Ÿ≠', 'Title 6 - Domestic Security', 'Title 7 - Agriculture', 'Title 12 - Banks and Banking', 'Title 15 - Commerce and Trade', 'Title 16 - Conservation', 'Title 19 - Customs Duties', 'Title 23 - Highways Ÿ≠', 'Title 25 - Indians', 'Title 26 - Internal Revenue Code', 'Title 29 - Labor', 'Title 30 - Mineral Lands and Mining', 'Title 33 - Navigation and Navigable Waters', 'Title 40 - Public Buildings, Property, and Works Ÿ≠', 'Title 41 - Public Contracts Ÿ≠', 'Title 42 - The Public Health and Welfare', 'Title 43 - Public Lands', 'Title 45 - Railroads', 'Title 46 - Shipping Ÿ≠', 'Title 47 - Telecommunications', 'Title 49 - Transportation Ÿ≠', 'Title 54 - National Park Service and Related Programs Ÿ≠'], 24 total changes since 12/27/21\n"
     ]
    }
   ],
   "source": [
    "list = driver.find_element(By.CLASS_NAME,'uscitemlist')\n",
    "titles = list.find_elements(By.CLASS_NAME,'uscitem')\n",
    "changes=[]\n",
    "for element in titles:\n",
    "    try:\n",
    "        bold = element.find_element(By.CLASS_NAME,'usctitlechanged').text #All changed titles are written in bold \n",
    "        changes.append(bold)\n",
    "    except:\n",
    "        pass\n",
    "print('{}, {} total changes since 12/27/21'.format(changes, len(changes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 6 - A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOSE RODOLFO VILLARREAL-HERNANDEZ\n",
      "RAFAEL CARO-QUINTERO\n",
      "YULAN ADONAY ARCHAGA CARIAS\n",
      "EUGENE PALMER\n",
      "BHADRESHKUMAR CHETANBHAI PATEL\n",
      "ALEJANDRO ROSALES CASTILLO\n",
      "ARNOLDO JIMENEZ\n",
      "JASON DEREK BROWN\n",
      "ALEXIS FLORES\n",
      "OCTAVIANO JUAREZ-CORRO\n"
     ]
    }
   ],
   "source": [
    "criminals = driver.find_elements(By.CLASS_NAME,'title')\n",
    "for i in range (1, len(criminals)): #First element in criminals is a text called most wantes, no needing to see it\n",
    "    print(criminals[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 7 - List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Espa√±ol / 1 717 000+ art√≠culos',\n",
       " 'English / 6 383 000+ articles',\n",
       " 'Êó•Êú¨Ë™û / 1 292 000+ Ë®ò‰∫ã',\n",
       " '–†—É—Å—Å–∫–∏–π / 1 756 000+ —Å—Ç–∞—Ç–µ–π',\n",
       " 'Deutsch / 2 617 000+ Artikel',\n",
       " 'Fran√ßais / 2 362 000+ articles',\n",
       " '‰∏≠Êñá / 1 231 000+ Ê¢ùÁõÆ',\n",
       " 'Italiano / 1 718 000+ voci',\n",
       " 'Polski / 1 490 000+ hase≈Ç',\n",
       " 'Portugu√™s / 1 074 000+ artigos']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wraper = driver.find_elements(By.CLASS_NAME,'central-featured')\n",
    "languages = driver.find_elements(By.CLASS_NAME,'link-box')\n",
    "lang_list = []\n",
    "for element in languages:\n",
    "    lang = element.find_element(By.TAG_NAME,'strong').text\n",
    "    articles = element.find_element(By.TAG_NAME,'small').text\n",
    "    lang_list.append(f'{lang} / {articles}')\n",
    "lang_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 8 - A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headings = driver.find_elements(By.CLASS_NAME,'dgu-topics__heading')\n",
    "topics = []\n",
    "for element in headings:\n",
    "    topics.append(element.find_element(By.TAG_NAME,'a').text)\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 9 - Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = driver.find_elements(By.CLASS_NAME,'wikitable')\n",
    "heads = table[1].find_element(By.TAG_NAME,'thead') #Second table in web page\n",
    "rows = table[1].find_elements(By.TAG_NAME,'tr')\n",
    "\n",
    "data = [[i.text for i in e.find_elements(By.TAG_NAME,'td')] for e in rows]\n",
    "index = [c.text for c in heads.find_elements(By.TAG_NAME,'th')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native\\nspeakers\\nin millions\\n2007 (2010)</th>\n",
       "      <th>Percentage\\nof world\\npopulation\\n(2007)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mandarin (entire branch)</td>\n",
       "      <td>935 (955)</td>\n",
       "      <td>14.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>390 (405)</td>\n",
       "      <td>5.85%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>365 (360)</td>\n",
       "      <td>5.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi[b]</td>\n",
       "      <td>295 (310)</td>\n",
       "      <td>4.46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>280 (295)</td>\n",
       "      <td>4.23%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>205 (215)</td>\n",
       "      <td>3.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>200 (205)</td>\n",
       "      <td>3.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Russian</td>\n",
       "      <td>160 (155)</td>\n",
       "      <td>2.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>125 (125)</td>\n",
       "      <td>1.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Punjabi</td>\n",
       "      <td>95 (100)</td>\n",
       "      <td>1.44%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Language Native\\nspeakers\\nin millions\\n2007 (2010)  \\\n",
       "1   Mandarin (entire branch)                                  935 (955)   \n",
       "2                    Spanish                                  390 (405)   \n",
       "3                    English                                  365 (360)   \n",
       "4                   Hindi[b]                                  295 (310)   \n",
       "5                     Arabic                                  280 (295)   \n",
       "6                 Portuguese                                  205 (215)   \n",
       "7                    Bengali                                  200 (205)   \n",
       "8                    Russian                                  160 (155)   \n",
       "9                   Japanese                                  125 (125)   \n",
       "10                   Punjabi                                   95 (100)   \n",
       "\n",
       "   Percentage\\nof world\\npopulation\\n(2007)  \n",
       "1                                     14.1%  \n",
       "2                                     5.85%  \n",
       "3                                     5.52%  \n",
       "4                                     4.46%  \n",
       "5                                     4.23%  \n",
       "6                                     3.08%  \n",
       "7                                     3.05%  \n",
       "8                                     2.42%  \n",
       "9                                     1.92%  \n",
       "10                                    1.44%  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Top_lang = pd.DataFrame(data, columns=index)\n",
    "Top_lang.dropna(how=\"all\",inplace=True)\n",
    "Top_lang.drop(axis = 1, columns='Rank', inplace=True)\n",
    "\n",
    "Top_lang.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping up the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Challenge 10 - The 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = driver.find_element(By.XPATH,'//*[@id=\"content\"]/table[2]')\n",
    "heads = table.find_element(By.TAG_NAME,'thead') #Second table in web page\n",
    "rows = table.find_elements(By.ID,'tbody')\n",
    "\n",
    "earthquakes = [[i.text for i in range(e.find_elements(By.TAG_NAME,'td'))] for e in rows]\n",
    "indices = [c.text for c in heads.find_elements(By.TAG_NAME,'th')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-15   00:02:39.2\\n27min ago',\n",
       "  '19.23 ',\n",
       "  'N  ',\n",
       "  '155.43 ',\n",
       "  'W  ',\n",
       "  '34',\n",
       "  '',\n",
       "  '2.1',\n",
       "  ' ISLAND OF HAWAII, HAWAII',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   23:56:26.0\\n33min ago',\n",
       "  '8.49 ',\n",
       "  'S  ',\n",
       "  '134.86 ',\n",
       "  'E  ',\n",
       "  '2',\n",
       "  '',\n",
       "  '4.9',\n",
       "  ' ARAFURA SEA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   23:53:11.0\\n36min ago',\n",
       "  '17.27 ',\n",
       "  'N  ',\n",
       "  '94.70 ',\n",
       "  'W  ',\n",
       "  '138',\n",
       "  '',\n",
       "  '4.1',\n",
       "  ' VERACRUZ, MEXICO',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  'F',\n",
       "  '2022-02-14   23:29:28.8\\n1hr 00min ago',\n",
       "  '32.06 ',\n",
       "  'N  ',\n",
       "  '102.22 ',\n",
       "  'W  ',\n",
       "  '5',\n",
       "  '',\n",
       "  '3.0',\n",
       "  ' WESTERN TEXAS',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   23:26:24.5\\n1hr 03min ago',\n",
       "  '19.06 ',\n",
       "  'N  ',\n",
       "  '67.17 ',\n",
       "  'W  ',\n",
       "  '10',\n",
       "  '',\n",
       "  '2.7',\n",
       "  ' PUERTO RICO REGION',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   23:17:50.3\\n1hr 12min ago',\n",
       "  '31.67 ',\n",
       "  'N  ',\n",
       "  '104.42 ',\n",
       "  'W  ',\n",
       "  '9',\n",
       "  '',\n",
       "  '2.8',\n",
       "  ' WESTERN TEXAS',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   23:14:47.0\\n1hr 15min ago',\n",
       "  '0.12 ',\n",
       "  'S  ',\n",
       "  '124.49 ',\n",
       "  'E  ',\n",
       "  '21',\n",
       "  '',\n",
       "  '4.3',\n",
       "  ' MOLUCCA SEA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   23:05:14.0\\n1hr 24min ago',\n",
       "  '13.03 ',\n",
       "  'N  ',\n",
       "  '87.54 ',\n",
       "  'W  ',\n",
       "  '5',\n",
       "  '',\n",
       "  '3.4',\n",
       "  ' NICARAGUA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   22:51:20.1\\n1hr 38min ago',\n",
       "  '25.50 ',\n",
       "  'S  ',\n",
       "  '178.56 ',\n",
       "  'E  ',\n",
       "  '596',\n",
       "  '',\n",
       "  '4.4',\n",
       "  ' SOUTH OF FIJI ISLANDS',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   22:40:28.5\\n1hr 49min ago',\n",
       "  '26.47 ',\n",
       "  'S  ',\n",
       "  '137.23 ',\n",
       "  'E  ',\n",
       "  '10',\n",
       "  '',\n",
       "  '2.9',\n",
       "  ' SOUTH AUSTRALIA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   22:38:53.4\\n1hr 51min ago',\n",
       "  '35.67 ',\n",
       "  'N  ',\n",
       "  '117.46 ',\n",
       "  'W  ',\n",
       "  '9',\n",
       "  '',\n",
       "  '2.0',\n",
       "  ' SOUTHERN CALIFORNIA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   22:32:38.0\\n1hr 57min ago',\n",
       "  '6.94 ',\n",
       "  'S  ',\n",
       "  '120.67 ',\n",
       "  'E  ',\n",
       "  '10',\n",
       "  '',\n",
       "  '3.4',\n",
       "  ' FLORES SEA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   22:20:58.4\\n2hr 09min ago',\n",
       "  '28.55 ',\n",
       "  'N  ',\n",
       "  '17.85 ',\n",
       "  'W  ',\n",
       "  '12',\n",
       "  '',\n",
       "  '1.5',\n",
       "  ' CANARY ISLANDS, SPAIN REGION',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   22:07:42.0\\n2hr 22min ago',\n",
       "  '34.62 ',\n",
       "  'S  ',\n",
       "  '72.77 ',\n",
       "  'W  ',\n",
       "  '8',\n",
       "  '',\n",
       "  '3.2',\n",
       "  ' OFFSHORE MAULE, CHILE',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   22:05:26.5\\n2hr 24min ago',\n",
       "  '43.11 ',\n",
       "  'N  ',\n",
       "  '0.42 ',\n",
       "  'W  ',\n",
       "  '10',\n",
       "  '',\n",
       "  '1.7',\n",
       "  ' PYRENEES',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   21:53:04.4\\n2hr 36min ago',\n",
       "  '70.82 ',\n",
       "  'N  ',\n",
       "  '14.04 ',\n",
       "  'W  ',\n",
       "  '15',\n",
       "  '',\n",
       "  '4.7',\n",
       "  ' JAN MAYEN ISLAND REGION',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   21:49:09.0\\n2hr 40min ago',\n",
       "  '23.84 ',\n",
       "  'S  ',\n",
       "  '67.23 ',\n",
       "  'W  ',\n",
       "  '258',\n",
       "  '',\n",
       "  '2.5',\n",
       "  ' SALTA, ARGENTINA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   21:45:14.0\\n2hr 44min ago',\n",
       "  '32.15 ',\n",
       "  'S  ',\n",
       "  '70.97 ',\n",
       "  'W  ',\n",
       "  '93',\n",
       "  '',\n",
       "  '2.5',\n",
       "  ' VALPARAISO, CHILE',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   21:39:23.9\\n2hr 50min ago',\n",
       "  '55.52 ',\n",
       "  'N  ',\n",
       "  '153.20 ',\n",
       "  'W  ',\n",
       "  '10',\n",
       "  '',\n",
       "  '3.2',\n",
       "  ' SOUTH OF ALASKA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   21:35:45.0\\n2hr 54min ago',\n",
       "  '31.21 ',\n",
       "  'S  ',\n",
       "  '68.07 ',\n",
       "  'W  ',\n",
       "  '81',\n",
       "  '',\n",
       "  '3.0',\n",
       "  ' SAN JUAN, ARGENTINA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   21:32:04.2\\n2hr 57min ago',\n",
       "  '70.76 ',\n",
       "  'N  ',\n",
       "  '14.01 ',\n",
       "  'W  ',\n",
       "  '10',\n",
       "  '',\n",
       "  '4.3',\n",
       "  ' JAN MAYEN ISLAND REGION',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   21:21:33.0\\n3hr 08min ago',\n",
       "  '21.18 ',\n",
       "  'S  ',\n",
       "  '68.97 ',\n",
       "  'W  ',\n",
       "  '105',\n",
       "  '',\n",
       "  '3.0',\n",
       "  ' TARAPACA, CHILE',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   21:17:35.9\\n3hr 12min ago',\n",
       "  '38.05 ',\n",
       "  'N  ',\n",
       "  '37.92 ',\n",
       "  'E  ',\n",
       "  '12',\n",
       "  '',\n",
       "  '2.3',\n",
       "  ' CENTRAL TURKEY',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   21:13:13.0\\n3hr 16min ago',\n",
       "  '5.14 ',\n",
       "  'N  ',\n",
       "  '95.58 ',\n",
       "  'E  ',\n",
       "  '16',\n",
       "  '',\n",
       "  '2.6',\n",
       "  ' NORTHERN SUMATRA, INDONESIA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   21:10:54.2\\n3hr 19min ago',\n",
       "  '3.36 ',\n",
       "  'S  ',\n",
       "  '130.99 ',\n",
       "  'E  ',\n",
       "  '10',\n",
       "  '',\n",
       "  '4.3',\n",
       "  ' SERAM, INDONESIA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   21:05:52.0\\n3hr 24min ago',\n",
       "  '23.86 ',\n",
       "  'S  ',\n",
       "  '67.33 ',\n",
       "  'W  ',\n",
       "  '233',\n",
       "  '',\n",
       "  '2.9',\n",
       "  ' ANTOFAGASTA, CHILE',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   20:47:47.0\\n3hr 42min ago',\n",
       "  '22.94 ',\n",
       "  'S  ',\n",
       "  '66.52 ',\n",
       "  'W  ',\n",
       "  '244',\n",
       "  '',\n",
       "  '3.7',\n",
       "  ' JUJUY, ARGENTINA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   20:39:13.1\\n3hr 50min ago',\n",
       "  '54.81 ',\n",
       "  'N  ',\n",
       "  '156.82 ',\n",
       "  'W  ',\n",
       "  '32',\n",
       "  '',\n",
       "  '3.8',\n",
       "  ' SOUTH OF ALASKA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   20:38:34.8\\n3hr 51min ago',\n",
       "  '40.74 ',\n",
       "  'N  ',\n",
       "  '34.17 ',\n",
       "  'E  ',\n",
       "  '4',\n",
       "  '',\n",
       "  '2.3',\n",
       "  ' CENTRAL TURKEY',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   20:37:29.0\\n3hr 52min ago',\n",
       "  '24.07 ',\n",
       "  'S  ',\n",
       "  '67.27 ',\n",
       "  'W  ',\n",
       "  '244',\n",
       "  '',\n",
       "  '2.9',\n",
       "  ' SALTA, ARGENTINA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   20:31:59.5\\n3hr 58min ago',\n",
       "  '19.23 ',\n",
       "  'N  ',\n",
       "  '155.42 ',\n",
       "  'W  ',\n",
       "  '35',\n",
       "  '',\n",
       "  '2.2',\n",
       "  ' ISLAND OF HAWAII, HAWAII',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   20:28:20.6\\n4hr 01min ago',\n",
       "  '70.74 ',\n",
       "  'N  ',\n",
       "  '14.03 ',\n",
       "  'W  ',\n",
       "  '2',\n",
       "  '',\n",
       "  '5.7',\n",
       "  ' JAN MAYEN ISLAND REGION',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   20:23:41.0\\n4hr 06min ago',\n",
       "  '36.01 ',\n",
       "  'N  ',\n",
       "  '27.64 ',\n",
       "  'E  ',\n",
       "  '8',\n",
       "  '',\n",
       "  '2.5',\n",
       "  ' DODECANESE IS.-TURKEY BORDER REG',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   20:22:36.0\\n4hr 07min ago',\n",
       "  '9.14 ',\n",
       "  'S  ',\n",
       "  '127.11 ',\n",
       "  'E  ',\n",
       "  '10',\n",
       "  '',\n",
       "  '4.2',\n",
       "  ' TIMOR SEA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   20:09:23.0\\n4hr 20min ago',\n",
       "  '39.71 ',\n",
       "  'N  ',\n",
       "  '39.57 ',\n",
       "  'E  ',\n",
       "  '13',\n",
       "  '',\n",
       "  '2.2',\n",
       "  ' EASTERN TURKEY',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   20:05:55.2\\n4hr 24min ago',\n",
       "  '54.62 ',\n",
       "  'N  ',\n",
       "  '161.12 ',\n",
       "  'W  ',\n",
       "  '15',\n",
       "  '',\n",
       "  '3.5',\n",
       "  ' ALASKA PENINSULA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   20:05:37.0\\n4hr 24min ago',\n",
       "  '24.18 ',\n",
       "  'S  ',\n",
       "  '67.55 ',\n",
       "  'W  ',\n",
       "  '217',\n",
       "  '',\n",
       "  '3.0',\n",
       "  ' SALTA, ARGENTINA',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  'III',\n",
       "  '2022-02-14   20:04:49.1\\n4hr 25min ago',\n",
       "  '38.09 ',\n",
       "  'N  ',\n",
       "  '22.61 ',\n",
       "  'E  ',\n",
       "  '2',\n",
       "  '',\n",
       "  '2.9',\n",
       "  ' GREECE',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   19:54:34.3\\n4hr 35min ago',\n",
       "  '43.51 ',\n",
       "  'N  ',\n",
       "  '0.59 ',\n",
       "  'W  ',\n",
       "  '10',\n",
       "  '',\n",
       "  '2.8',\n",
       "  ' PYRENEES',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   19:52:28.3\\n4hr 37min ago',\n",
       "  '38.04 ',\n",
       "  'N  ',\n",
       "  '37.96 ',\n",
       "  'E  ',\n",
       "  '8',\n",
       "  '',\n",
       "  '2.0',\n",
       "  ' CENTRAL TURKEY',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   19:27:49.0\\n5hr 02min ago',\n",
       "  '24.06 ',\n",
       "  'S  ',\n",
       "  '67.01 ',\n",
       "  'W  ',\n",
       "  '191',\n",
       "  '',\n",
       "  '3.6',\n",
       "  ' SALTA, ARGENTINA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   19:17:39.8\\n5hr 12min ago',\n",
       "  '28.85 ',\n",
       "  'N  ',\n",
       "  '130.17 ',\n",
       "  'E  ',\n",
       "  '30',\n",
       "  '',\n",
       "  '4.7',\n",
       "  ' RYUKYU ISLANDS, JAPAN',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   19:08:07.0\\n5hr 21min ago',\n",
       "  '38.85 ',\n",
       "  'S  ',\n",
       "  '176.71 ',\n",
       "  'E  ',\n",
       "  '12',\n",
       "  '',\n",
       "  '3.8',\n",
       "  ' NORTH ISLAND OF NEW ZEALAND',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   19:03:51.5\\n5hr 26min ago',\n",
       "  '36.99 ',\n",
       "  'N  ',\n",
       "  '30.14 ',\n",
       "  'E  ',\n",
       "  '9',\n",
       "  '',\n",
       "  '2.0',\n",
       "  ' WESTERN TURKEY',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   18:15:55.0\\n6hr 14min ago',\n",
       "  '19.22 ',\n",
       "  'S  ',\n",
       "  '69.49 ',\n",
       "  'W  ',\n",
       "  '91',\n",
       "  '',\n",
       "  '2.9',\n",
       "  ' TARAPACA, CHILE',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   18:15:25.0\\n6hr 14min ago',\n",
       "  '3.75 ',\n",
       "  'S  ',\n",
       "  '120.94 ',\n",
       "  'E  ',\n",
       "  '114',\n",
       "  '',\n",
       "  '2.7',\n",
       "  ' SULAWESI, INDONESIA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   18:13:27.0\\n6hr 16min ago',\n",
       "  '13.18 ',\n",
       "  'N  ',\n",
       "  '90.49 ',\n",
       "  'W  ',\n",
       "  '13',\n",
       "  '',\n",
       "  '3.9',\n",
       "  ' OFFSHORE GUATEMALA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   18:12:19.3\\n6hr 17min ago',\n",
       "  '60.58 ',\n",
       "  'N  ',\n",
       "  '147.66 ',\n",
       "  'W  ',\n",
       "  '17',\n",
       "  '',\n",
       "  '3.1',\n",
       "  ' SOUTHERN ALASKA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   18:09:08.0\\n6hr 20min ago',\n",
       "  '11.77 ',\n",
       "  'N  ',\n",
       "  '87.27 ',\n",
       "  'W  ',\n",
       "  '22',\n",
       "  '',\n",
       "  '2.9',\n",
       "  ' NEAR COAST OF NICARAGUA',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '2022-02-14   18:03:38.2\\n6hr 26min ago',\n",
       "  '33.35 ',\n",
       "  'S  ',\n",
       "  '116.99 ',\n",
       "  'E  ',\n",
       "  '5',\n",
       "  '',\n",
       "  '2.4',\n",
       "  ' WESTERN AUSTRALIA',\n",
       "  '']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 11 - IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 12 - Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 13 - Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(city):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 14 - Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did you limit your output? Thank you! üôÇ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
